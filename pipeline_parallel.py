"""
Pipeline Parallelism for overlapping computation stages
"""
import torch
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor, Future
from typing import List, Dict, Any, Callable, Optional
import numpy as np

class PipelineStage:
    """Represents a single stage in the pipeline"""
    
    def __init__(self, name: str, func: Callable, device: str = "cuda"):
        self.name = name
        self.func = func
        self.device = device
        self.input_queue = queue.Queue(maxsize=2)
        self.output_queue = queue.Queue(maxsize=2)
        self.thread = None
        self.running = False
        self.profiling_data = {
            "total_time": 0,
            "num_calls": 0,
            "avg_time": 0
        }
    
    def start(self):
        """Start the pipeline stage thread"""
        self.running = True
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.thread.start()
    
    def stop(self):
        """Stop the pipeline stage"""
        self.running = False
        self.input_queue.put(None)  # Sentinel value
        if self.thread:
            self.thread.join(timeout=5.0)
    
    def _worker(self):
        """Worker thread that processes inputs"""
        while self.running:
            try:
                item = self.input_queue.get(timeout=0.1)
                if item is None:  # Sentinel value
                    break
                
                start_time = time.time()
                
                # Process the item
                with torch.cuda.device(self.device) if "cuda" in self.device else torch.cpu.device():
                    result = self.func(item)
                
                # Update profiling data
                elapsed = time.time() - start_time
                self.profiling_data["total_time"] += elapsed
                self.profiling_data["num_calls"] += 1
                self.profiling_data["avg_time"] = self.profiling_data["total_time"] / self.profiling_data["num_calls"]
                
                # Put result in output queue
                self.output_queue.put(result)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Error in pipeline stage {self.name}: {e}")
                self.output_queue.put(e)  # Pass exception forward


class ParallelPipeline:
    """Manages parallel execution of pipeline stages"""
    
    def __init__(self, num_devices: int = 1):
        self.stages: List[PipelineStage] = []
        self.num_devices = num_devices
        self.executor = ThreadPoolExecutor(max_workers=8)
        self.profiling_enabled = False
    
    def add_stage(self, name: str, func: Callable, device: str = "cuda:0"):
        """Add a new stage to the pipeline"""
        stage = PipelineStage(name, func, device)
        self.stages.append(stage)
        return stage
    
    def start(self):
        """Start all pipeline stages"""
        for stage in self.stages:
            stage.start()
    
    def stop(self):
        """Stop all pipeline stages"""
        for stage in self.stages:
            stage.stop()
        self.executor.shutdown(wait=True)
    
    def process_batch(self, items: List[Any]) -> List[Any]:
        """Process a batch of items through the pipeline"""
        results = []
        futures = []
        
        # Submit all items to the first stage
        for item in items:
            future = self.executor.submit(self._process_single, item)
            futures.append(future)
        
        # Collect results
        for future in futures:
            result = future.result()
            results.append(result)
        
        return results
    
    def _process_single(self, item: Any) -> Any:
        """Process a single item through all stages"""
        current_item = item
        
        for i, stage in enumerate(self.stages):
            # Put item in stage's input queue
            stage.input_queue.put(current_item)
            
            # Get result from stage's output queue
            result = stage.output_queue.get()
            
            # Check if it's an exception
            if isinstance(result, Exception):
                raise result
            
            current_item = result
        
        return current_item
    
    def get_profiling_report(self) -> Dict[str, Any]:
        """Get profiling information for all stages"""
        if not self.profiling_enabled:
            return {}
        
        report = {}
        total_time = sum(stage.profiling_data["total_time"] for stage in self.stages)
        
        for stage in self.stages:
            report[stage.name] = {
                "avg_time": stage.profiling_data["avg_time"],
                "total_time": stage.profiling_data["total_time"],
                "percentage": (stage.profiling_data["total_time"] / total_time * 100) if total_time > 0 else 0,
                "num_calls": stage.profiling_data["num_calls"]
            }
        
        return report


class LatentSyncParallelPipeline:
    """Specialized parallel pipeline for LatentSync"""
    
    def __init__(self, pipeline, gpu_info: dict):
        self.pipeline = pipeline
        self.gpu_info = gpu_info
        self.parallel_pipeline = None
        
        # Determine parallelization strategy based on GPU
        self.use_multi_stream = gpu_info.get("vram_gb", 0) >= 12
        self.num_streams = 3 if gpu_info.get("vram_gb", 0) >= 24 else 2
        
        # Create CUDA streams for parallel execution
        if self.use_multi_stream and torch.cuda.is_available():
            self.streams = [torch.cuda.Stream() for _ in range(self.num_streams)]
        else:
            self.streams = []
    
    def create_parallel_stages(self):
        """Create parallel processing stages for LatentSync"""
        self.parallel_pipeline = ParallelPipeline()
        
        # Stage 1: Face detection and preprocessing
        def face_processing_stage(frame_batch):
            """Process faces in parallel with main pipeline"""
            faces = []
            boxes = []
            affine_matrices = []
            
            for frame in frame_batch:
                try:
                    # This would use the actual face processing logic
                    face_info = self.pipeline.image_processor.get_face_info(frame)
                    faces.append(face_info['face'])
                    boxes.append(face_info['box'])
                    affine_matrices.append(face_info['affine_matrix'])
                except Exception as e:
                    # Handle face detection failure
                    faces.append(None)
                    boxes.append(None)
                    affine_matrices.append(None)
            
            return {
                'frames': frame_batch,
                'faces': faces,
                'boxes': boxes,
                'affine_matrices': affine_matrices
            }
        
        # Stage 2: VAE encoding (can overlap with face processing)
        def vae_encoding_stage(data):
            """Encode frames to latent space"""
            frames = data['frames']
            faces = data['faces']
            
            # Process valid faces
            valid_faces = [f for f in faces if f is not None]
            if valid_faces:
                with torch.cuda.stream(self.streams[0]) if self.streams else torch.cuda.device(0):
                    # Batch encode faces
                    face_tensor = torch.stack(valid_faces)
                    latents = self.pipeline.vae.encode(face_tensor).latent_dist.sample()
                    data['latents'] = latents
            else:
                data['latents'] = None
            
            return data
        
        # Stage 3: UNet denoising (main computation)
        def unet_stage(data):
            """Run UNet denoising"""
            if data['latents'] is None:
                return data
            
            with torch.cuda.stream(self.streams[1]) if len(self.streams) > 1 else torch.cuda.device(0):
                # This would run the actual denoising loop
                # Simplified for demonstration
                data['denoised_latents'] = data['latents']  # Placeholder
            
            return data
        
        # Stage 4: VAE decoding (can start before all frames are denoised)
        def vae_decoding_stage(data):
            """Decode latents back to image space"""
            if data.get('denoised_latents') is None:
                return data
            
            with torch.cuda.stream(self.streams[2]) if len(self.streams) > 2 else torch.cuda.device(0):
                # Decode latents
                decoded = self.pipeline.vae.decode(data['denoised_latents']).sample
                data['decoded_faces'] = decoded
            
            return data
        
        # Add stages to pipeline
        self.parallel_pipeline.add_stage("face_processing", face_processing_stage)
        self.parallel_pipeline.add_stage("vae_encoding", vae_encoding_stage)
        self.parallel_pipeline.add_stage("unet", unet_stage)
        self.parallel_pipeline.add_stage("vae_decoding", vae_decoding_stage)
        
        return self.parallel_pipeline
    
    def process_video_parallel(self, video_frames: List[np.ndarray], 
                              audio_features: torch.Tensor,
                              batch_size: int = 4) -> List[np.ndarray]:
        """Process video frames in parallel batches"""
        
        if not self.parallel_pipeline:
            self.create_parallel_stages()
        
        self.parallel_pipeline.start()
        
        try:
            # Process frames in batches
            processed_frames = []
            
            for i in range(0, len(video_frames), batch_size):
                batch = video_frames[i:i + batch_size]
                
                # Process batch through parallel pipeline
                results = self.parallel_pipeline.process_batch(batch)
                
                # Extract processed frames
                for result in results:
                    if 'decoded_faces' in result:
                        processed_frames.extend(result['decoded_faces'])
                    else:
                        # Fallback to original frames if processing failed
                        processed_frames.extend(result['frames'])
            
            # Get profiling report
            if self.parallel_pipeline.profiling_enabled:
                report = self.parallel_pipeline.get_profiling_report()
                print("\nðŸ“Š Pipeline Profiling Report:")
                for stage_name, stats in report.items():
                    print(f"  {stage_name}: {stats['avg_time']:.3f}s avg ({stats['percentage']:.1f}%)")
            
            return processed_frames
            
        finally:
            self.parallel_pipeline.stop()
    
    def enable_stream_parallelism(self):
        """Enable CUDA stream-based parallelism for overlapping operations"""
        if not torch.cuda.is_available():
            return
        
        print(f"ðŸš€ Enabling {self.num_streams}-stream parallelism")
        
        # Create events for synchronization
        self.events = [[torch.cuda.Event() for _ in range(2)] for _ in self.streams]
        
        # Warm up streams
        for stream in self.streams:
            with torch.cuda.stream(stream):
                torch.cuda.empty_cache()
    
    def parallel_face_vae_processing(self, faces: List[torch.Tensor]) -> List[torch.Tensor]:
        """Process faces through VAE encoder in parallel streams"""
        if not self.streams or len(faces) < 2:
            # Fallback to sequential processing
            return [self.pipeline.vae.encode(face).latent_dist.sample() for face in faces]
        
        # Distribute work across streams
        results = [None] * len(faces)
        faces_per_stream = len(faces) // len(self.streams)
        
        for i, stream in enumerate(self.streams):
            start_idx = i * faces_per_stream
            end_idx = start_idx + faces_per_stream if i < len(self.streams) - 1 else len(faces)
            
            with torch.cuda.stream(stream):
                for j in range(start_idx, end_idx):
                    results[j] = self.pipeline.vae.encode(faces[j]).latent_dist.sample()
        
        # Synchronize all streams
        for stream in self.streams:
            stream.synchronize()
        
        return results


def create_parallel_optimizer(pipeline, gpu_info: dict):
    """Create a parallel optimization wrapper for the pipeline"""
    return LatentSyncParallelPipeline(pipeline, gpu_info)


def enable_pipeline_parallelism(pipeline, gpu_info: dict):
    """Enable pipeline parallelism optimizations"""
    
    print("ðŸ”„ Enabling pipeline parallelism...")
    
    # Create parallel optimizer
    parallel_opt = create_parallel_optimizer(pipeline, gpu_info)
    
    # Monkey-patch the pipeline to use parallel processing
    original_call = pipeline.__call__
    
    def parallel_call(self, *args, **kwargs):
        # Check if we should use parallel processing
        video_frames = kwargs.get('video_frames', [])
        if len(video_frames) > 10 and parallel_opt.use_multi_stream:
            print("ðŸ“ˆ Using parallel pipeline processing")
            # Enable stream parallelism
            parallel_opt.enable_stream_parallelism()
            
            # TODO: Integrate with actual pipeline call
            # This would require modifying the pipeline internals
        
        # Fallback to original call
        return original_call(self, *args, **kwargs)
    
    pipeline.__call__ = parallel_call.__get__(pipeline, pipeline.__class__)
    
    print("âœ… Pipeline parallelism enabled")
    
    return pipeline